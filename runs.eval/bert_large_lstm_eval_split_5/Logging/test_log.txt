Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 18,
 'CONFIG_NAME': 'bert_large_lstm_eval_split_5',
 'CROSS_VALIDATION_FLAG': False,
 'CUDA': True,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EXPERIMENT_NAME': 'bert_large_lstm_eval_split_5',
 'GLOVE_DIM': 100,
 'GPU_NUM': 1,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': True,
          'BIDIRECTION': True,
          'DROP_PROB': 0.3,
          'FLAG': True,
          'HIDDEN_DIM': 800,
          'LAYERS': 2,
          'SEQ_LEN': 30},
 'MODE': 'test',
 'OUT_PATH': '/jagupard21/scr1/sebschu/eval/',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'test',
 'RESUME_DIR': '',
 'SAVE_PREDS': True,
 'SEED': 1,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './some_database.csv',
 'SPLIT_NAME': '5',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': False,
           'INTERVAL': 190,
           'LR': 0.001,
           'LR_DECAY_EPOCH': 100,
           'LR_DECAY_RATE': 1.0,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 190}}
Using random seed 1.
Path to the current word embeddings: ./datasets/seed_1/5/bert_largelayer_18_lstm/embs_test_30.npy
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /sailhome/sebschu/.cache/torch/pytorch_transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /sailhome/sebschu/.cache/torch/pytorch_transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38
Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /sailhome/sebschu/.cache/torch/pytorch_transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6
epochs to test: [0, 1, 190]
initializing neural net
Load from: /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Model/RNet_epoch_0.pth
Write attention weights to /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Attention/test_attn_epoch0.npy.
initializing neural net
Load from: /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Model/RNet_epoch_1.pth
Write attention weights to /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Attention/test_attn_epoch1.npy.
initializing neural net
Load from: /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Model/RNet_epoch_190.pth
Write attention weights to /jagupard21/scr1/sebschu/eval/bert_large_lstm_eval_split_5/Attention/test_attn_epoch190.npy.
Max r = 0.7825783804883409 achieved at epoch 190
r by epoch: [-0.12315441759222984, 0.7585419784238062, 0.7825783804883409]
